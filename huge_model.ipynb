{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d47ae-f709-4488-bab3-6113aa33eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "# to display all the columns of the dataframe in the notebook\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002faa2-e46e-4cfe-9b76-bf96bc1bf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# evaluate model and separate train and test\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d8e46-02fb-4a18-9389-8f76f0f5fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6634666-9b8d-47de-87a9-7f6a2823ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the convolutional network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e63e6-b14a-48bd-b0e6-fb33f8e93e07",
   "metadata": {},
   "source": [
    "# Load Images / Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8bfed-6d5d-413d-b382-16a8ed639371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all our images\n",
    "DATA_FOLDER = 'v2-plant-seedling-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd82a2-97d1-4d9b-9771-2ce68a3138c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each wee class is in a dedicated folder\n",
    "os.listdir(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bdb02-7067-4ce8-8ea3-5b290f12487f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's walk over the directory structure, so we understand\n",
    "# how the images are stored\n",
    "\n",
    "for class_folder_name in os.listdir(DATA_FOLDER):\n",
    "    class_folder_path = os.path.join(DATA_FOLDER, class_folder_name)\n",
    "    for image_path in glob(os.path.join(class_folder_path, '*.png')):\n",
    "        print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab27b98-3f1d-48bb-a089-201ea9990110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's creare a dataframe:\n",
    "# the dataframe stores the path to the image in one column\n",
    "# and the class of the weed (the target) in the next column\n",
    "\n",
    "images_df = []\n",
    "\n",
    "# navigate within each folder\n",
    "for class_folder_name in os.listdir(DATA_FOLDER):\n",
    "    class_folder_path = os.path.join(DATA_FOLDER, class_folder_name)\n",
    "\n",
    "    # collect everry image path\n",
    "    for image_path in glob(os.path.join(class_folder_path, '*.png')):\n",
    "        tmp = pd.DataFrame([image_path, class_folder_name]).T\n",
    "        images_df.append(tmp)\n",
    "\n",
    "# concatenate the final df\n",
    "images_df = pd.concat(images_df, axis=0, ignore_index=True)\n",
    "images_df.columns = ['image', 'target']\n",
    "images_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24af90b-9e41-433a-a63e-2b768ebe5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images do we've got per class?\n",
    "# this should be give similar results to what we observerd\n",
    "# when we inspected the length of image listin the dictionary\n",
    "\n",
    "images_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6db477-17e8-4c31-88c8-f433b5339674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's isolate a path, for demo\n",
    "# we want to load the image in this path later\n",
    "\n",
    "images_df.loc[0, 'image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02969ad-d1fd-470c-9a43-66b528a4c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise a few images\n",
    "# if the images you see in your notebook are not the same, don't\n",
    "\n",
    "def plot_single_image(df, image_number):\n",
    "    im = cv2.imread(df.loc[image_number, 'image'])\n",
    "    plt.title(df.loc[image_number, 'target'])\n",
    "    plt.imshow(im)\n",
    "\n",
    "plot_single_image(images_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f1ed8-2ea9-47c8-ad6f-3beee35ff3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_image(images_df, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5ec3f-dc23-4a1c-8b10-3d1c2a095d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_image(images_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d328551-025d-4157-b9fc-e7f38fbfbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot's go ahead and plot a bunch of imges together, \n",
    "# so we get a better filing of how out images look like\n",
    "\n",
    "def plot_for_class(df, label):\n",
    "    # function plots 9 images\n",
    "    nb_rows = 3\n",
    "    nb_cols = 3\n",
    "\n",
    "    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 10))\n",
    "\n",
    "    n = 0\n",
    "    for i in range(0, nb_rows):\n",
    "        for j in range(0, nb_cols):\n",
    "            tmp = df[df['target'] == label]\n",
    "            tmp.reset_index(drop=True, inplace=True)\n",
    "            im = cv2.imread(tmp.loc[n, 'image'])\n",
    "            axs[i, j].imshow(im)\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af8aa4-1b83-482e-aa4d-022daaf76605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(images_df, 'Cleavers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e918b0a-b81c-447e-a325-77dd34cf8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(images_df, 'Maize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6759b4f-017e-4bb1-861d-c71bde206f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(images_df, 'Common Chickweed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc21e5-177e-49bd-bc4f-60b921655f9e",
   "metadata": {},
   "source": [
    "# Separate train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acade4a0-01ad-4a3e-82a8-8bfaa2d5209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_df['image'], images_df['target'], \n",
    "                                                    test_size=.20, random_state=101)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62110074-089a-4077-8696-dc1ea8198217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the indeces of the training data are mixed\n",
    "# this will couse problems later\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a5492-4cf0-4ee3-9ddf-25e2e9905032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index, because later we iterate over row number\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d2e4f-915c-4a9e-b2a9-08430a196f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index in target as well\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a298b-e386-4cb6-b8b8-1998aafc7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of images within each class\n",
    "\n",
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb983fb-8cb6-4745-b512-01044ed808b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the same in the test set\n",
    "\n",
    "y_test.value_counts() / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee652f17-3976-4c17-a2f6-d5ebe303cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the target\n",
    "# it is a multiclass classification, so we need to make\n",
    "# one hot encoding of ther target\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "train_y = to_categorical(encoder.transform(y_train))\n",
    "test_y = to_categorical(encoder.transform(y_test))\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b04351-299a-46f9-a296-983a1b48179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images in our folders, are all different sizes\n",
    "# For neural networks however, we need images in the same size\n",
    "# The imges will all be resized to this size:\n",
    "\n",
    "IMAGE_SIZE = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49897f75-a11c-46e3-b080-cc5871228bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_resize(df, n):\n",
    "    im = cv2.imread(df[n])\n",
    "    im = cv2.resize(im, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e1d9b-8b6e-4f01-8c19-8cf6d25f0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = im_resize(X_train, 7)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3a74c-fa0a-4cce-a9d6-e22bd1339e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the datasets needs to be (n1, n2, n3, n4)\n",
    "# where n1 is the number of observations\n",
    "# n2 and n3 are image width length\n",
    "# and n4 indecates that it is a color image, so 3 planes per image\n",
    "\n",
    "def create_dataset(df, image_size):\n",
    "    # functions creates dataset as required for cnn\n",
    "    tmp = np.zeros((len(df), image_size, image_size, 3), dtype='float32')\n",
    "\n",
    "    for n in range(0, len(df)):\n",
    "        im = im_resize(df, n)\n",
    "        tmp[n] = im\n",
    "\n",
    "    print(f'Dataset Images shape: {tmp.shape} size {tmp.size:,}')\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6b28e-964d-4df3-9dd7-9d2916e5a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = create_dataset(X_train, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264cdc-87e1-4308-b25c-d64a7b852fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = create_dataset(X_test, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9591b-df6d-43c2-bcfe-e7a5e584b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different classes\n",
    "\n",
    "len(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b13e4b-f928-4e5b-9dae-a4e5b1c7ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our cnn\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = .3\n",
    "dropout_dense = .3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(first_filters, kernel_size, activation='relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52c10d-e725-413e-966d-9e52331f7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=.0001), loss='binary_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf7853-d1bf-4fb1-9bd8-af2872ad38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd3861-8586-42ac-b4bb-b22c356cab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model.keras'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1,\n",
    "                            save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=.5, patience=1, verbose=1,\n",
    "                             mode='max', min_lr=.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(x=x_train, y=train_y, \n",
    "                   batch_size=batch_size,\n",
    "                   validation_split=10,\n",
    "                   epochs=epochs,\n",
    "                   verbose=2,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea72fe-5b01-4f59-a437-960582ae028b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
